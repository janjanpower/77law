#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Excelåˆ†æå™¨ - å°ˆè²¬Excelæ™ºæ…§åˆ†æåŠŸèƒ½
æä¾›å·¥ä½œè¡¨åˆ†é¡ã€æ¬„ä½å°æ‡‰ã€è³‡æ–™å“è³ªåˆ†æç­‰åŠŸèƒ½
"""

import re
from typing import Dict, List, Optional, Any, Tuple

# å®‰å…¨çš„ä¾è³´å°å…¥
try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False

from models.case_model import CaseData
from utils.data_cleaner import DataCleaner
from .excel_reader import ExcelReader
from .exceptions import ExcelDependencyError


class ExcelAnalyzer:
    """Excelåˆ†æå™¨é¡åˆ¥"""

    def __init__(self):
        """åˆå§‹åŒ–Excelåˆ†æå™¨"""
        self._check_dependencies()
        self.reader = ExcelReader()

        # æ¬„ä½å°æ‡‰è¦å‰‡
        self.field_mapping_rules = {
            'client': ['ç•¶äº‹äºº', 'å®¢æˆ¶', 'å§”è¨—äºº', 'å§“å', 'åç¨±'],
            'case_reason': ['æ¡ˆç”±', 'äº‹ç”±', 'æ¡ˆä»¶äº‹ç”±'],
            'case_number': ['æ¡ˆè™Ÿ', 'æ©Ÿé—œ', 'æ¡ˆä»¶è™Ÿç¢¼', 'æ©Ÿé—œæ¡ˆè™Ÿ', 'æ³•é™¢æ¡ˆè™Ÿ', 'æ¡ˆä»¶è™Ÿ', 'è™Ÿç¢¼'],
            'court': ['æ³•é™¢', 'è² è²¬æ³•é™¢', 'ç®¡è½„æ³•é™¢', 'å¯©ç†æ³•é™¢', 'æ³•åº­'],
            'opposing_party': ['å°é€ ', 'å°æ–¹ç•¶äº‹äºº', 'ç›¸å°äºº'],
            'division': ['è‚¡åˆ¥', 'è² è²¬è‚¡åˆ¥', 'æ‰¿è¾¦è‚¡'],
            'lawyer': ['å¾‹å¸«', 'ä»£ç†äºº', 'å§”ä»»å¾‹å¸«'],
            'legal_affairs': ['æ³•å‹™', 'æ³•å‹™äººå“¡'],
            'progress': ['é€²åº¦', 'ç‹€æ…‹', 'æ¡ˆä»¶ç‹€æ…‹'],
            'case_id': ['æ¡ˆä»¶ç·¨è™Ÿ', 'ç·¨è™Ÿ', 'ID', 'åºè™Ÿ']
        }

        # æ¡ˆä»¶é¡å‹è­˜åˆ¥é—œéµå­—
        self.case_type_keywords = {
            'æ°‘äº‹': ['æ°‘äº‹', 'civil', 'æ°‘', 'å¥‘ç´„', 'ä¾µæ¬Š', 'å‚µå‹™', 'è²¡ç”¢'],
            'åˆ‘äº‹': ['åˆ‘äº‹', 'criminal', 'åˆ‘', 'çŠ¯ç½ª', 'é•æ³•', 'èµ·è¨´'],
            'è¡Œæ”¿': ['è¡Œæ”¿', 'administrative', 'è¡Œæ”¿è¨´è¨Ÿ', 'è¡Œæ”¿è™•åˆ†'],
            'å®¶äº‹': ['å®¶äº‹', 'family', 'é›¢å©š', 'ç›£è­·', 'æ”¶é¤Š']
        }

    def _check_dependencies(self) -> None:
        """æª¢æŸ¥å¿…è¦ä¾è³´"""
        if not PANDAS_AVAILABLE:
            raise ExcelDependencyError("pandas ä¸å¯ç”¨ï¼Œç„¡æ³•é€²è¡ŒExcelåˆ†æ")

    def analyze_excel_comprehensive(self, file_path: str) -> Tuple[bool, str, Dict[str, Any]]:
        """
        ç¶œåˆåˆ†æExcelæª”æ¡ˆ

        Args:
            file_path: Excelæª”æ¡ˆè·¯å¾‘

        Returns:
            (æˆåŠŸç‹€æ…‹, åˆ†æå ±å‘Š, è©³ç´°çµæœ)
        """
        try:
            print(f"ğŸ” é–‹å§‹ç¶œåˆåˆ†æ: {file_path}")

            # è®€å–æª”æ¡ˆåŸºæœ¬è³‡è¨Š
            file_info = self.reader.read_excel_file_info(file_path)
            print(f"    æª”æ¡ˆå¤§å°: {file_info['file_size_mb']} MB")
            print(f"    å·¥ä½œè¡¨æ•¸é‡: {file_info['sheet_count']}")

            # åˆ†ææ‰€æœ‰å·¥ä½œè¡¨
            sheets_analysis = {}
            categorized_sheets = {'æ°‘äº‹': [], 'åˆ‘äº‹': [], 'è¡Œæ”¿': [], 'å®¶äº‹': [], 'unknown': []}

            for sheet_name in file_info['sheet_names']:
                print(f"  ğŸ” åˆ†æå·¥ä½œè¡¨: {sheet_name}")

                sheet_analysis = self.analyze_single_sheet(file_path, sheet_name)
                sheets_analysis[sheet_name] = sheet_analysis

                # åˆ†é¡å·¥ä½œè¡¨
                if sheet_analysis['success']:
                    case_type = self._classify_sheet_by_content(sheet_name, sheet_analysis)
                    categorized_sheets[case_type].append(sheet_name)
                    print(f"      åˆ†é¡çµæœ: {case_type}")
                else:
                    categorized_sheets['unknown'].append(sheet_name)
                    print(f"      åˆ†æå¤±æ•—: {sheet_analysis.get('error', 'æœªçŸ¥éŒ¯èª¤')}")

            # ç”Ÿæˆåˆ†æå ±å‘Š
            report = self._generate_analysis_report(file_info, sheets_analysis, categorized_sheets)

            # æ§‹å»ºè©³ç´°çµæœ
            detailed_result = {
                'file_info': file_info,
                'sheets_analysis': sheets_analysis,
                'categorized_sheets': categorized_sheets,
                'analysis_summary': {
                    'total_sheets': len(file_info['sheet_names']),
                    'processable_sheets': sum(1 for analysis in sheets_analysis.values() if analysis['success']),
                    'case_types_found': {k: len(v) for k, v in categorized_sheets.items() if v}
                }
            }

            success = any(analysis['success'] for analysis in sheets_analysis.values())
            print("âœ… ç¶œåˆåˆ†æå®Œæˆ")

            return success, report, detailed_result

        except Exception as e:
            error_msg = f"ç¶œåˆåˆ†æå¤±æ•—: {str(e)}"
            print(f"âŒ {error_msg}")
            return False, error_msg, {}

    def analyze_single_sheet(self, file_path: str, sheet_name: str) -> Dict[str, Any]:
        """
        åˆ†æå–®ä¸€å·¥ä½œè¡¨

        Args:
            file_path: Excelæª”æ¡ˆè·¯å¾‘
            sheet_name: å·¥ä½œè¡¨åç¨±

        Returns:
            å·¥ä½œè¡¨åˆ†æçµæœ
        """
        try:
            # è®€å–å·¥ä½œè¡¨è³‡æ–™
            df = self.reader.read_sheet_data(file_path, sheet_name)

            if df is None or df.empty:
                return {
                    'success': False,
                    'error': 'å·¥ä½œè¡¨ç‚ºç©ºæˆ–ç„¡æ³•è®€å–',
                    'has_data': False
                }

            # æª¢æ¸¬æ¨™é¡Œåˆ—
            header_row = self.reader.detect_header_row(file_path, sheet_name)

            # åˆ†ææ¬„ä½å°æ‡‰
            column_mapping = self._create_column_mapping(df.columns.tolist())

            # åˆ†æè³‡æ–™å“è³ª
            quality_analysis = self._analyze_data_quality(df)

            # æª¢æŸ¥æ¡ˆè™Ÿåˆä½µéœ€æ±‚
            merge_analysis = self._analyze_case_number_merge_needs(df.columns.tolist())

            # é ä¼°è³‡æ–™ç­†æ•¸
            valid_data_count = self._estimate_valid_data_count(df, column_mapping)

            return {
                'success': True,
                'sheet_name': sheet_name,
                'header_row': header_row,
                'total_rows': len(df),
                'total_columns': len(df.columns),
                'column_mapping': column_mapping,
                'quality_analysis': quality_analysis,
                'merge_analysis': merge_analysis,
                'valid_data_count': valid_data_count,
                'has_required_fields': bool(column_mapping.get('client')),
                'columns': df.columns.tolist()
            }

        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'has_data': False
            }

    def extract_cases_from_analysis(
        self,
        file_path: str,
        analysis_result: Dict[str, Any]
    ) -> Tuple[bool, str, Dict[str, List[CaseData]]]:
        """
        æ ¹æ“šåˆ†æçµæœæå–æ¡ˆä»¶è³‡æ–™

        Args:
            file_path: Excelæª”æ¡ˆè·¯å¾‘
            analysis_result: åˆ†æçµæœ

        Returns:
            (æˆåŠŸç‹€æ…‹, çµæœè¨Šæ¯, åˆ†é¡æ¡ˆä»¶è³‡æ–™)
        """
        try:
            all_cases = {'æ°‘äº‹': [], 'åˆ‘äº‹': [], 'è¡Œæ”¿': [], 'å®¶äº‹': []}
            import_summary = []

            sheets_analysis = analysis_result.get('sheets_analysis', {})
            categorized_sheets = analysis_result.get('categorized_sheets', {})

            for case_type, sheet_names in categorized_sheets.items():
                if case_type == 'unknown' or not sheet_names:
                    continue

                for sheet_name in sheet_names:
                    sheet_analysis = sheets_analysis.get(sheet_name)
                    if not sheet_analysis or not sheet_analysis['success']:
                        continue

                    try:
                        print(f"  æå– {case_type} æ¡ˆä»¶: {sheet_name}")
                        cases = self._extract_cases_from_sheet(
                            file_path, sheet_name, case_type, sheet_analysis
                        )

                        if cases:
                            all_cases[case_type].extend(cases)
                            import_summary.append(f"ã€Œ{sheet_name}ã€: {len(cases)} ç­†{case_type}æ¡ˆä»¶")
                            print(f"    âœ… æå– {len(cases)} ç­†")
                        else:
                            import_summary.append(f"ã€Œ{sheet_name}ã€: æ²’æœ‰æœ‰æ•ˆè³‡æ–™")

                    except Exception as e:
                        import_summary.append(f"ã€Œ{sheet_name}ã€: æå–å¤±æ•— - {str(e)}")
                        print(f"    âŒ æå–å¤±æ•—: {e}")

            # çµ±è¨ˆçµæœ
            total_cases = sum(len(cases) for cases in all_cases.values())

            if total_cases == 0:
                return False, "æ²’æœ‰æˆåŠŸæå–ä»»ä½•æ¡ˆä»¶è³‡æ–™", all_cases

            # ç”Ÿæˆçµæœè¨Šæ¯
            summary_lines = [f"âœ… æå–å®Œæˆï¼å…± {total_cases} ç­†æ¡ˆä»¶"]

            for case_type, cases in all_cases.items():
                if cases:
                    summary_lines.append(f"ğŸ“‹ {case_type}æ¡ˆä»¶: {len(cases)} ç­†")

            summary_lines.append("\nğŸ“Š è©³ç´°çµæœ:")
            summary_lines.extend(import_summary)

            return True, "\n".join(summary_lines), all_cases

        except Exception as e:
            error_msg = f"æå–æ¡ˆä»¶è³‡æ–™å¤±æ•—: {str(e)}"
            print(f"âŒ {error_msg}")
            return False, error_msg, {'æ°‘äº‹': [], 'åˆ‘äº‹': [], 'è¡Œæ”¿': [], 'å®¶äº‹': []}

    def _create_column_mapping(self, columns: List[str]) -> Dict[str, str]:
        """å»ºç«‹æ¬„ä½å°æ‡‰é—œä¿‚"""
        mapping = {}
        used_columns = set()

        print(f"        å¯ç”¨æ¬„ä½: {', '.join(str(col) for col in columns)}")

        # æŒ‰å„ªå…ˆé †åºå°æ‡‰æ¬„ä½
        for field, keywords in self.field_mapping_rules.items():
            matched_column = self._find_best_column_match(field, columns, used_columns)
            if matched_column:
                mapping[field] = matched_column
                used_columns.add(matched_column)
                print(f"        âœ… {field}: ã€Œ{matched_column}ã€")

        return mapping

    def _find_best_column_match(self, field: str, columns: List[str], used_columns: set) -> Optional[str]:
        """å°‹æ‰¾æœ€ä½³æ¬„ä½åŒ¹é…"""
        keywords = self.field_mapping_rules.get(field, [])
        best_match = None
        best_score = 0

        for col in columns:
            if col in used_columns:
                continue

            col_clean = str(col).strip()
            score = 0

            # ç²¾ç¢ºåŒ¹é…å¾—åˆ†æœ€é«˜
            for keyword in keywords:
                if keyword == col_clean:
                    score += 100
                elif keyword in col_clean:
                    score += 50
                elif col_clean in keyword:
                    score += 30
                elif self._fuzzy_match(keyword, col_clean):
                    score += 20

            if score > best_score:
                best_score = score
                best_match = col

        return best_match if best_score > 0 else None

    def _fuzzy_match(self, keyword: str, column: str) -> bool:
        """æ¨¡ç³ŠåŒ¹é…"""
        # ç§»é™¤æ¨™é»ç¬¦è™Ÿå’Œç©ºç™½
        keyword_clean = re.sub(r'[^\w]', '', keyword)
        column_clean = re.sub(r'[^\w]', '', column)

        # æª¢æŸ¥æ˜¯å¦æœ‰å…±åŒå­—å…ƒ
        common_chars = set(keyword_clean) & set(column_clean)
        return len(common_chars) >= min(2, len(keyword_clean) // 2)

    def _classify_sheet_by_content(self, sheet_name: str, sheet_analysis: Dict[str, Any]) -> str:
        """æ ¹æ“šå…§å®¹åˆ†é¡å·¥ä½œè¡¨"""
        # å…ˆæ ¹æ“šå·¥ä½œè¡¨åç¨±åˆ†é¡
        sheet_name_lower = sheet_name.lower()

        for case_type, keywords in self.case_type_keywords.items():
            if any(keyword in sheet_name_lower for keyword in keywords):
                return case_type

        # å¦‚æœåç¨±ç„¡æ³•åˆ¤æ–·ï¼Œæ ¹æ“šæ¬„ä½å…§å®¹åˆ†æ
        columns = sheet_analysis.get('columns', [])
        column_text = ' '.join(str(col).lower() for col in columns)

        for case_type, keywords in self.case_type_keywords.items():
            if any(keyword in column_text for keyword in keywords):
                return case_type

        return 'unknown'

    def _analyze_data_quality(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æè³‡æ–™å“è³ª"""
        quality_info = {
            'total_rows': len(df),
            'non_empty_rows': 0,
            'columns_info': {},
            'quality_score': 0
        }

        # åˆ†ææ¯å€‹æ¬„ä½
        for col in df.columns:
            non_null_count = df[col].notna().sum()
            null_count = df[col].isna().sum()

            quality_info['columns_info'][col] = {
                'non_null_count': non_null_count,
                'null_count': null_count,
                'fill_rate': non_null_count / len(df) if len(df) > 0 else 0,
                'unique_count': df[col].nunique(),
                'data_type': str(df[col].dtype)
            }

        # è¨ˆç®—éç©ºè¡Œæ•¸
        quality_info['non_empty_rows'] = len(df.dropna(how='all'))

        # è¨ˆç®—å“è³ªåˆ†æ•¸ï¼ˆ0-100ï¼‰
        if len(df) > 0:
            avg_fill_rate = sum(info['fill_rate'] for info in quality_info['columns_info'].values()) / len(quality_info['columns_info'])
            quality_info['quality_score'] = round(avg_fill_rate * 100, 2)

        return quality_info

    def _analyze_case_number_merge_needs(self, columns: List[str]) -> Dict[str, Any]:
        """åˆ†ææ¡ˆè™Ÿæ¬„ä½åˆä½µéœ€æ±‚"""
        case_number_related = []

        for col in columns:
            col_clean = str(col).strip().lower()
            if any(keyword in col_clean for keyword in ['æ¡ˆè™Ÿ', 'æ©Ÿé—œ', 'è™Ÿç¢¼', 'å­—è™Ÿ']):
                case_number_related.append(col)

        needs_merge = len(case_number_related) > 1

        return {
            'needs_merge': needs_merge,
            'case_number_fields': case_number_related,
            'merge_separator': '-' if needs_merge else '',
            'merge_description': f"ç™¼ç¾ {len(case_number_related)} å€‹æ¡ˆè™Ÿç›¸é—œæ¬„ä½ï¼Œå»ºè­°åˆä½µ" if needs_merge else "å–®ä¸€æ¡ˆè™Ÿæ¬„ä½"
        }

    def _estimate_valid_data_count(self, df: pd.DataFrame, column_mapping: Dict[str, str]) -> int:
        """é ä¼°æœ‰æ•ˆè³‡æ–™ç­†æ•¸"""
        if not column_mapping.get('client'):
            return 0

        client_column = column_mapping['client']
        if client_column not in df.columns:
            return 0

        # è¨ˆç®—ç•¶äº‹äººæ¬„ä½ä¸­æœ‰æ•ˆè³‡æ–™çš„è¡Œæ•¸
        valid_count = df[client_column].notna().sum()
        return int(valid_count)

    def _extract_cases_from_sheet(
        self,
        file_path: str,
        sheet_name: str,
        case_type: str,
        sheet_analysis: Dict[str, Any]
    ) -> List[CaseData]:
        """å¾å·¥ä½œè¡¨æå–æ¡ˆä»¶è³‡æ–™"""
        try:
            # è®€å–å·¥ä½œè¡¨è³‡æ–™
            df = self.reader.read_sheet_data(file_path, sheet_name)
            if df is None or df.empty:
                return []

            column_mapping = sheet_analysis['column_mapping']
            merge_info = sheet_analysis['merge_analysis']

            if not column_mapping.get('client'):
                print(f"      æ‰¾ä¸åˆ°ç•¶äº‹äººæ¬„ä½")
                return []

            cases = []
            processed_rows = 0

            for index, row in df.iterrows():
                try:
                    # æå–åŸºæœ¬è³‡æ–™
                    case_data = {
                        'case_type': case_type,
                        'client': self._safe_extract_value(row, column_mapping.get('client')),
                        'case_id': self._safe_extract_value(row, column_mapping.get('case_id')),
                        'case_reason': self._safe_extract_value(row, column_mapping.get('case_reason')),
                        'lawyer': self._safe_extract_value(row, column_mapping.get('lawyer')),
                        'legal_affairs': self._safe_extract_value(row, column_mapping.get('legal_affairs')),
                        'opposing_party': self._safe_extract_value(row, column_mapping.get('opposing_party')),
                        'court': self._safe_extract_value(row, column_mapping.get('court')),
                        'division': self._safe_extract_value(row, column_mapping.get('division'))
                    }

                    # è™•ç†æ¡ˆè™Ÿåˆä½µ
                    if merge_info['needs_merge']:
                        case_number_parts = []
                        for field in merge_info['case_number_fields']:
                            part = self._safe_extract_value(row, field)
                            if part:
                                case_number_parts.append(part)
                        case_data['case_number'] = merge_info['merge_separator'].join(case_number_parts) if case_number_parts else None
                    else:
                        case_data['case_number'] = self._safe_extract_value(row, column_mapping.get('case_number'))

                    # æª¢æŸ¥å¿…è¦æ¬„ä½
                    if not case_data['client']:
                        processed_rows += 1
                        continue

                    # å»ºç«‹æ¡ˆä»¶ç‰©ä»¶
                    case = CaseData(
                        case_type=case_data['case_type'],
                        client=case_data['client'],
                        case_id=case_data['case_id'],
                        case_reason=case_data['case_reason'],
                        case_number=case_data['case_number'],
                        court=case_data['court'],
                        division=case_data['division'],
                        lawyer=case_data['lawyer'],
                        legal_affairs=case_data['legal_affairs'],
                        opposing_party=case_data['opposing_party'],
                        progress='å¾…è™•ç†'
                    )

                    cases.append(case)
                    processed_rows += 1

                except Exception as e:
                    print(f"      è™•ç†ç¬¬ {index + 1} è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    continue

            return cases

        except Exception as e:
            print(f"âŒ å¾å·¥ä½œè¡¨ {sheet_name} æå–è³‡æ–™å¤±æ•—: {e}")
            return []

    def _safe_extract_value(self, row, column_name: str) -> Optional[str]:
        """å®‰å…¨æå–æ¬„ä½å€¼ä¸¦æ¸…ç†"""
        if not column_name:
            return None

        try:
            value = row.get(column_name, None)
            if pd.isna(value):
                return None

            # ä½¿ç”¨çµ±ä¸€çš„è³‡æ–™æ¸…ç†å·¥å…·
            cleaned_value = DataCleaner.clean_text_data(value)
            return cleaned_value

        except Exception:
            return None

    def _generate_analysis_report(
        self,
        file_info: Dict[str, Any],
        sheets_analysis: Dict[str, Any],
        categorized_sheets: Dict[str, List[str]]
    ) -> str:
        """ç”Ÿæˆåˆ†æå ±å‘Š"""
        report_lines = []

        # æª”æ¡ˆåŸºæœ¬è³‡è¨Š
        report_lines.append(f"ğŸ“„ æª”æ¡ˆ: {file_info['file_path']}")
        report_lines.append(f"ğŸ“Š å·¥ä½œè¡¨ç¸½æ•¸: {file_info['sheet_count']}")

        # å¯è™•ç†å·¥ä½œè¡¨çµ±è¨ˆ
        processable_count = sum(1 for analysis in sheets_analysis.values() if analysis['success'])
        report_lines.append(f"âœ… å¯è™•ç†å·¥ä½œè¡¨: {processable_count}")

        # æ¡ˆä»¶é¡å‹åˆ†å¸ƒ
        for case_type, sheets in categorized_sheets.items():
            if sheets and case_type != 'unknown':
                report_lines.append(f"ğŸ“‹ {case_type}å·¥ä½œè¡¨ {len(sheets)} å€‹: {', '.join(sheets)}")

        # è³‡æ–™å“è³ªæ‘˜è¦
        total_valid_cases = 0
        for sheet_name, analysis in sheets_analysis.items():
            if analysis['success']:
                total_valid_cases += analysis.get('valid_data_count', 0)

        report_lines.append(f"ğŸ“ˆ é ä¼°å¯åŒ¯å…¥æ¡ˆä»¶: {total_valid_cases} ç­†")

        return "\n".join(report_lines)